{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcdf006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4fa0626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/12 19:29:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/12 19:30:01 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7168f570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|seat_id|free|\n",
      "+-------+----+\n",
      "|      1|   1|\n",
      "|      2|   0|\n",
      "|      3|   1|\n",
      "|      4|   1|\n",
      "|      5|   1|\n",
      "+-------+----+\n",
      "\n",
      "[('seat_id', 'bigint'), ('free', 'bigint')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "data = [Row(seat_id=1, free=1),\n",
    "        Row(seat_id=2, free=0),\n",
    "        Row(seat_id=3, free=1),\n",
    "        Row(seat_id=4, free=1),\n",
    "        Row(seat_id=5, free=1)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b25b3e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"cinema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c5e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/12 12:51:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/12/12 12:51:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/12/12 12:51:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/12/12 12:51:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/12/12 12:51:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|seat_id|\n",
      "+-------+\n",
      "|      3|\n",
      "|      4|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\n",
    "    \"\"\"\n",
    "    -- wrong solution, missing the lass seat_id\n",
    "    Select seat_id \n",
    "    From (Select *, lead(free) over(order by seat_id) as next from cinema ) r\n",
    "    Where free = 1 and next = 1\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf95406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/12 12:55:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/12/12 12:55:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/12/12 12:55:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|seat_id|\n",
      "+-------+\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/12 12:55:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/12/12 12:55:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\n",
    "    \"\"\"          \n",
    "    -- wrong solution, missing the lass seat_id\n",
    "    Select seat_id \n",
    "    From (Select *, lead(free) over(order by seat_id) as next, lag(free) over(order by seat_id) as prev from cinema ) r\n",
    "    Where (free = 1 and next = 1) or(free = 1 and prev = 1)\n",
    "    \"\"\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f11db4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=============================================>          (52 + 8) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|seat_id|\n",
      "+-------+\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:===================================================>    (59 + 5) / 64]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\n",
    "    \"\"\"          \n",
    "    -- duplicates\n",
    "    Select distinct a.seat_id\n",
    "    From cinema a join cinema b on (a.seat_id + 1 = b.seat_id) or a.seat_id = b.seat_id + 1\n",
    "    Where (a.free = 1 and b.free = 1) \n",
    "    Order by a.seat_id\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd62aab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=================================================>      (57 + 7) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|seat_id|\n",
      "+-------+\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\n",
    "    \"\"\"          \n",
    "    Select distinct a.seat_id\n",
    "    From cinema a join cinema b on abs(a.seat_id - b.seat_id) = 1\n",
    "    Where (a.free = 1 and b.free = 1) \n",
    "    Order by a.seat_id\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5562d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'month': 'jan', 'customerId': '101', 'sales': 105}, {'month': 'feb', 'customerId': '101', 'sales': 106}, {'month': 'feb', 'customerId': '101', 'sales': 200}, {'month': 'jan', 'customerId': '100', 'sales': 50}, {'month': 'feb', 'customerId': '100', 'sales': 100}]\n"
     ]
    }
   ],
   "source": [
    "rows = [\n",
    "    { 'month': 'jan', 'customerId': '101', 'sales': 105 },\n",
    "    { 'month': 'feb', 'customerId': '101', 'sales': 106 },\n",
    "    { 'month': 'feb', 'customerId': '101', 'sales': 200 },\n",
    "    { 'month': 'jan', 'customerId': '100', 'sales': 50 },\n",
    "    { 'month': 'feb', 'customerId': '100', 'sales': 100 }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a9a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jan': 1, 'feb': 2}\n"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "for row in rows:\n",
    "    if row['sales'] >= 100:\n",
    "        result[row['month']] = result.get(row['month'], set())\n",
    "        result[row['month']].add(row['customerId'])\n",
    "\n",
    "print({k:len(v) for k,v in result.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "518839fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|month|customerId|sales|\n",
      "+-----+----------+-----+\n",
      "|  jan|       101|  105|\n",
      "|  feb|       101|  106|\n",
      "|  feb|       101|  200|\n",
      "|  jan|       100|   50|\n",
      "|  feb|       100|  100|\n",
      "+-----+----------+-----+\n",
      "\n",
      "[('month', 'string'), ('customerId', 'string'), ('sales', 'bigint')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "data = [Row(**row) for row in rows]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "print(df.dtypes)\n",
    "df.createOrReplaceTempView(\"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef7e15c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|month|count|\n",
      "+-----+-----+\n",
      "|  feb|    2|\n",
      "|  jan|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\n",
    "    \"\"\"          \n",
    "    SELECT\n",
    "    month,\n",
    "    COUNT(DISTINCT(customerId)) as count\n",
    "FROM\n",
    "    rows\n",
    "WHERE sales >= 100\n",
    "GROUP BY month;\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42c5df9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|month|count|\n",
      "+-----+-----+\n",
      "|  feb|    2|\n",
      "|  jan|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\n",
    "    \"\"\"          \n",
    "    SELECT\n",
    "    month,\n",
    "    COUNT(DISTINCT(case when sales >= 100 then customerId else null end)) as count\n",
    "    FROM rows\n",
    "    GROUP BY month;\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
